{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 20:16:17 WARN Utils: Your hostname, Abhijits-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.105 instead (on interface en0)\n",
      "24/04/09 20:16:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/09 20:16:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/09 20:16:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/09 20:16:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 20:16:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Analyzing air line data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = sc.parallelize([Row(id=1,\n",
    "                        name =\"Jill\",\n",
    "                        active = True,\n",
    "                        clubs = [\"chess\", \"hockey\"],\n",
    "                        subjects ={\"math\" : 80, \"english\": 56},\n",
    "                        enrolled = datetime(2014,8,1,14,1,5)),\n",
    "                        Row(id=2,\n",
    "                        name =\"George\",\n",
    "                        active = True,\n",
    "                        clubs = [\"chess\", \"soccer\"],\n",
    "                        subjects ={\"math\" : 60, \"english\": 96},\n",
    "                        enrolled = datetime(2015,3,21,8,2,5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "record_df = record.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To run sql queries on dataframe we need to register this dataframe as a table\n",
    "\n",
    "record_df.createOrReplaceTempView(\"records\")  ### Create temp table per-session -> once session ends this table goes away. also not sharable across sessions\n",
    "\n",
    "#### In order to acheive the sharing use createOrReplaceGlobalView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhijit/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running queries with SQL context\n",
    "\n",
    "all_records_df = sqlContext.sql(\"SELECT * from Records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------------+--------------------+-------------------+\n",
      "| id|  name|active|          clubs|            subjects|           enrolled|\n",
      "+---+------+------+---------------+--------------------+-------------------+\n",
      "|  1|  Jill|  true|[chess, hockey]|{english -> 56, m...|2014-08-01 14:01:05|\n",
      "|  2|George|  true|[chess, soccer]|{english -> 96, m...|2015-03-21 08:02:05|\n",
      "+---+------+------+---------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_records_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------------+\n",
      "| id|clubs[1]|subjects[english]|\n",
      "+---+--------+-----------------+\n",
      "|  1|  hockey|               56|\n",
      "|  2|  soccer|               96|\n",
      "+---+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Complex queries\n",
    "sqlContext.sql(\"SELECT id, clubs[1], subjects['english'] from Records\").show()  ## Fetches according to requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---------------+--------------------+-------------------+\n",
      "| id|  name|active|          clubs|            subjects|           enrolled|\n",
      "+---+------+------+---------------+--------------------+-------------------+\n",
      "|  2|George|  true|[chess, soccer]|{english -> 96, m...|2015-03-21 08:02:05|\n",
      "+---+------+------+---------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We can use queries as: \n",
    "sqlContext.sql(\"SELECT * from Records where subjects['english']>90\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In order to access global tables we need to add a \"global_temp\" prefix to the table name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema in spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('active', BooleanType(), True), StructField('clubs', ArrayType(StringType(), True), True), StructField('subjects', MapType(StringType(), LongType(), True), True), StructField('enrolled', TimestampType(), True)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df.schema\n",
    "\n",
    "### Even if we do not define a schema externally Spark SQL infers a schema from a dataframe -> inferred schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Emily', '44', '55', '78'],\n",
       " ['Andy', '47', '34', '89'],\n",
       " ['Rick', '55', '78', '55'],\n",
       " ['Aaron', '66', '34', '98']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also explicitly define schema from outside\n",
    "\n",
    "lines = sc.textFile(\"/Volumes/T7/GettingStartedSpark2/students.txt\")\n",
    "parts = lines.map(lambda line: line.split(','))\n",
    "\n",
    "parts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaString = \"name math english science\"\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "fields = [StructField(\"name\", StringType() , True),\n",
    "StructField(\"math\", LongType() , True),\n",
    "StructField(\"english\", LongType() , True),\n",
    "StructField(\"science\", LongType() , True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaStudents = spark.createDataFrame(parts, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('name', StringType(), True), StructField('math', LongType(), True), StructField('english', LongType(), True), StructField('science', LongType(), True)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaStudents.schema   ## Thus we can define schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Window functions\n",
    "#### rowsBetween(-1, 1) -> for row x it considers value from the row before x to the row after x\n",
    "\n",
    "### rangeBetween(0, 50) -> this ranges from the current row to the 50th row from current row\n",
    "\n",
    "### rangeBetween(0, sys.maxsize) -> goes from that row to the last row of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Volumes/T7/GettingStartedSpark2/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|   product|category|price|\n",
      "+----------+--------+-----+\n",
      "|Samsung TX|  Tablet|  999|\n",
      "|Samsung JX|  Mobile|  799|\n",
      "|Redmi Note|  Mobile|  399|\n",
      "|        Mi|  Mobile|  299|\n",
      "|      iPad|  Tablet|  789|\n",
      "|    iPhone|  Mobile|  999|\n",
      "|  Micromax|  Mobile|  249|\n",
      "|    Lenovo|  Tablet|  499|\n",
      "|   OnePlus|  Mobile|  356|\n",
      "|        Xu|  Tablet|  267|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec1 = Window.partitionBy(products[\"category\"]).orderBy(products[\"price\"].desc())\n",
    "\n",
    "## We partition our product based on category -> i.e, mobile  or tablet and then within each partition we order by price in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_rank = func.rank().over(windowSpec1)\n",
    "\n",
    "### Func ranks each element in the window by prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Using rowsBetween\n",
    "\n",
    "windowSpec2 = Window.partitionBy(products[\"category\"]).orderBy(products[\"price\"].desc()).rowsBetween(-1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
