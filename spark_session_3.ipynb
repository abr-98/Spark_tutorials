{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 17:48:07 WARN Utils: Your hostname, Abhijits-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.105 instead (on interface en0)\n",
      "24/04/09 17:48:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/09 17:48:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 17:48:19 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Analyzing Soccer players\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Player details\n",
    "\n",
    "player = spark.read\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .load(\"/Volumes/T7/GettingStartedSpark2/player.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------------+------------------+-------------------+------+------+\n",
      "| id|player_api_id|       player_name|player_fifa_api_id|           birthday|height|weight|\n",
      "+---+-------------+------------------+------------------+-------------------+------+------+\n",
      "|  1|       505942|Aaron Appindangoye|            218353|1992-02-29 00:00:00|182.88|   187|\n",
      "|  2|       155782|   Aaron Cresswell|            189615|1989-12-15 00:00:00|170.18|   146|\n",
      "|  3|       162549|       Aaron Doran|            186170|1991-05-13 00:00:00|170.18|   163|\n",
      "|  4|        30572|     Aaron Galindo|            140161|1982-05-08 00:00:00|182.88|   198|\n",
      "|  5|        23780|      Aaron Hughes|             17725|1979-11-08 00:00:00|182.88|   154|\n",
      "+---+-------------+------------------+------------------+-------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- player_api_id: string (nullable = true)\n",
      " |-- player_name: string (nullable = true)\n",
      " |-- player_fifa_api_id: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Player Attributes\n",
    "\n",
    "player_attributes = spark.read\\\n",
    "            .format(\"csv\")\\\n",
    "            .option(\"header\",\"true\")\\\n",
    "            .load(\"/Volumes/T7/GettingStartedSpark2/Player_Attributes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- player_fifa_api_id: string (nullable = true)\n",
      " |-- player_api_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- overall_rating: string (nullable = true)\n",
      " |-- potential: string (nullable = true)\n",
      " |-- preferred_foot: string (nullable = true)\n",
      " |-- attacking_work_rate: string (nullable = true)\n",
      " |-- defensive_work_rate: string (nullable = true)\n",
      " |-- crossing: string (nullable = true)\n",
      " |-- finishing: string (nullable = true)\n",
      " |-- heading_accuracy: string (nullable = true)\n",
      " |-- short_passing: string (nullable = true)\n",
      " |-- volleys: string (nullable = true)\n",
      " |-- dribbling: string (nullable = true)\n",
      " |-- curve: string (nullable = true)\n",
      " |-- free_kick_accuracy: string (nullable = true)\n",
      " |-- long_passing: string (nullable = true)\n",
      " |-- ball_control: string (nullable = true)\n",
      " |-- acceleration: string (nullable = true)\n",
      " |-- sprint_speed: string (nullable = true)\n",
      " |-- agility: string (nullable = true)\n",
      " |-- reactions: string (nullable = true)\n",
      " |-- balance: string (nullable = true)\n",
      " |-- shot_power: string (nullable = true)\n",
      " |-- jumping: string (nullable = true)\n",
      " |-- stamina: string (nullable = true)\n",
      " |-- strength: string (nullable = true)\n",
      " |-- long_shots: string (nullable = true)\n",
      " |-- aggression: string (nullable = true)\n",
      " |-- interceptions: string (nullable = true)\n",
      " |-- positioning: string (nullable = true)\n",
      " |-- vision: string (nullable = true)\n",
      " |-- penalties: string (nullable = true)\n",
      " |-- marking: string (nullable = true)\n",
      " |-- standing_tackle: string (nullable = true)\n",
      " |-- sliding_tackle: string (nullable = true)\n",
      " |-- gk_diving: string (nullable = true)\n",
      " |-- gk_handling: string (nullable = true)\n",
      " |-- gk_kicking: string (nullable = true)\n",
      " |-- gk_positioning: string (nullable = true)\n",
      " |-- gk_reflexes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "player_attributes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11060"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking if the details are of same player:\n",
    "\n",
    "player_attributes.select(\"player_api_id\")\\\n",
    "        .distinct()\\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11060"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.select(\"player_api_id\")\\\n",
    "        .distinct()\\\n",
    "            .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_api_id', 'player_name', 'birthday', 'height', 'weight']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player = player.drop('id', 'player_fifa_api_id')\n",
    "player.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_api_id',\n",
       " 'date',\n",
       " 'overall_rating',\n",
       " 'finishing',\n",
       " 'heading_accuracy',\n",
       " 'volleys',\n",
       " 'dribbling',\n",
       " 'curve',\n",
       " 'free_kick_accuracy',\n",
       " 'long_passing',\n",
       " 'ball_control',\n",
       " 'acceleration',\n",
       " 'agility',\n",
       " 'reactions',\n",
       " 'shot_power',\n",
       " 'stamina',\n",
       " 'strength',\n",
       " 'long_shots',\n",
       " 'interceptions',\n",
       " 'positioning',\n",
       " 'vision',\n",
       " 'penalties',\n",
       " 'marking',\n",
       " 'standing_tackle',\n",
       " 'sliding_tackle',\n",
       " 'gk_diving',\n",
       " 'gk_handling',\n",
       " 'gk_kicking',\n",
       " 'gk_positioning',\n",
       " 'gk_reflexes']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_attributes = player_attributes.drop('id', \n",
    "                     'player_fifa_api_id',\n",
    "                     'preferred_foot',\n",
    "                     'attacking_work_rate',\n",
    "                     'defensive_work_rate',\n",
    "                     'crossing',\n",
    "                     'jumping',\n",
    "                     'sprint_speed',\n",
    "                     'balance',\n",
    "                     'aggression',\n",
    "                     'short_passing','potential')\n",
    "player_attributes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined functions -> UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apart from the functions present in the spark.sql.functions it lets us create our own user defined functions (udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_extract_udf = udf(lambda date: str(date).split('-')[0])    ### Defining a function\n",
    "\n",
    "player_attributes = player_attributes.withColumn(\n",
    "    \"year\",\n",
    "    year_extract_udf(player_attributes.date)    ### Using func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_attributes = player_attributes.drop(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_2016  = player_attributes.filter(player_attributes.year == 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Unique players\n",
    "\n",
    "pa_striker_2016 = pa_2016.groupBy(\"player_api_id\")\\\n",
    "    .agg({\"finishing\": \"avg\", \"shot_power\": \"avg\", \"acceleration\": \"avg\"})\n",
    "    \n",
    "### Using multiple aggregation at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-----------------+---------------+\n",
      "|player_api_id|   avg(finishing)|avg(acceleration)|avg(shot_power)|\n",
      "+-------------+-----------------+-----------------+---------------+\n",
      "|       309726|75.44444444444444|74.11111111111111|           76.0|\n",
      "|        26112|             53.0|             51.0|           76.0|\n",
      "|        38433|            68.25|             74.0|           74.0|\n",
      "|        38576|             61.0|             59.0|           74.0|\n",
      "|       438780|             49.0|             78.0|           57.0|\n",
      "+-------------+-----------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pa_striker_2016.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_striker_2016 = pa_striker_2016.withColumnRenamed(\"avg(finishing)\", \"finishing\")\\\n",
    "    .withColumnRenamed(\"avg(acceleration)\", \"acceleration\")\\\n",
    "    .withColumnRenamed(\"avg(shot_power)\", \"shot_power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Weight of each prop for selection\n",
    "\n",
    "weight_finishing = 1\n",
    "weight_shot_power = 2\n",
    "weight_acceleration = 1\n",
    "\n",
    "total_weight = weight_finishing +weight_shot_power + weight_acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grading strikers\n",
    "\n",
    "strikers = pa_striker_2016.withColumn(\"striker_grade\", (pa_striker_2016.finishing * weight_finishing + \\\n",
    "                                       pa_striker_2016.acceleration * weight_acceleration + \\\n",
    "                                       pa_striker_2016.shot_power * weight_shot_power) / total_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "strikers = strikers.drop(\"acceleration\", \"shot_power\", \"finishing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|player_api_id|    striker_grade|\n",
      "+-------------+-----------------+\n",
      "|       309726|75.38888888888889|\n",
      "|        26112|             64.0|\n",
      "|        38433|          72.5625|\n",
      "|        38576|             67.0|\n",
      "|       438780|            60.25|\n",
      "+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strikers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|player_api_id|striker_grade|\n",
      "+-------------+-------------+\n",
      "|        20276|        89.25|\n",
      "|        37412|         89.0|\n",
      "|        38817|        88.75|\n",
      "|        32118|        88.25|\n",
      "|        31921|         87.0|\n",
      "+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### sorting and filtering\n",
    "\n",
    "strikers = strikers.filter(strikers.striker_grade > 70) \\\n",
    "    .sort(strikers.striker_grade.desc())\n",
    "    \n",
    "strikers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Join \n",
    "\n",
    "#striker_details = player.join(strikers, player.player_api_id == strikers.player_api_id)\n",
    "\n",
    "## OR\n",
    "\n",
    "striker_details = player.join(strikers, [\"player_api_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_api_id',\n",
       " 'player_name',\n",
       " 'birthday',\n",
       " 'height',\n",
       " 'weight',\n",
       " 'striker_grade']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "striker_details.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform join operations by broadcasting dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "### Now the broadcast variables are stored in the cache, so we can not save too large data to broadcast\n",
    "\n",
    "### As strikers df is smaller, we broadcast that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "striker_details = player.select(\"player_api_id\", \"player_name\").join(broadcast(strikers), [\"player_api_id\"], 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyzing heading accuracy with height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_heading_acc = player_attributes.select(\"player_api_id\", \"heading_accuracy\").join(\n",
    "    broadcast(player),\n",
    "    [\"player_api_id\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['player_api_id',\n",
       " 'heading_accuracy',\n",
       " 'player_name',\n",
       " 'birthday',\n",
       " 'height',\n",
       " 'weight']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_heading_acc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we bucket the players according to height\n",
    "\n",
    "### Defining accumulator to maintain count\n",
    "\n",
    "short_count = spark.sparkContext.accumulator(0)\n",
    "medium_low_count = spark.sparkContext.accumulator(0)\n",
    "medium_high_count = spark.sparkContext.accumulator(0)\n",
    "tall_count = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_players_by_height(row):\n",
    "    height = float(row[\"height\"])\n",
    "    \n",
    "    if height <= 175:\n",
    "        short_count.add(1)\n",
    "    elif height > 175 and height <=183:\n",
    "        medium_low_count.add(1)        ## Adding to accumulator\n",
    "    elif height > 183 and height <=195:\n",
    "        medium_high_count.add(1)\n",
    "    elif height > 195:\n",
    "        tall_count.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_heading_acc.foreach(lambda x : count_players_by_height(x))\n",
    "\n",
    "## Foreach method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38408, 197916, 124822, 6810]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_players  = [short_count.value, medium_low_count.value, medium_high_count.value, tall_count.value]\n",
    "all_players "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data as CSV and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pa_2016.select(\"player_api_id\", \"overall_rating\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"players_overall.csv\")\n",
    "    \n",
    "    #### coalesce joins the data distributed across all workers to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import  AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "       return [0.0] * len(value)\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in range(len(val1)):\n",
    "            val1[i] += val2[i]\n",
    "            return val1\n",
    "va = spark.sparkContext.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
